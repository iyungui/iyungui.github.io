# 3주차 파이널 프로젝트 (네이버 영화 리뷰)

종류: 파이널 프로젝트
태그: 3주차
학습일: 2023년 1월 31일 → 2023년 2월 8일

# [시작하기 전에] 데이터 분석 목적 및 소개

---

> 소개: 영화 <기생충>을 바탕으로 네이버 영화 리뷰 자연어 처리 및 분석을 하고자 합니다.
> 

> 학습 목표: Python 패키지인 requests와 bs4를 활용하여 웹 크롤링을 통해 데이터를 수집할 수 있습니다. 한국어 형태소 분석기인 KoNLPy의 구문을 활용하여 한국어를 형태소 단위로 분할하고, 그 결과를 분석 데이터로 활용합니다. sklearn 패키지에서 제공하는 함수를 활용하여, 토픽 모델링과 감성 분석 과정을 체험할 수 있습니다.
> 

# [Part 1] 네이버 영화 리뷰 크롤링

---

### 네이버 영화 리뷰 페이지에 접속하기

```python
# requests 패키지 불러오기 (pip install requests 설치 필요)
import requests

# 네이버 영화 사이트 - 기생충 리뷰 페이지 
url = "https://movie.naver.com/movie/bi/mi/review.naver?code=161967"

# requests.get
resp = requests.get(url)
```

### BeautifulSoup 으로 HTML 해석하기

```python
# BeautifulSoup 함수 불러오기 (pip install bs4 설치 필요)  
from bs4 import BeautifulSoup 

# BeautifulSoup 함수로, HTML 문서 구조를 파싱 
soup = BeautifulSoup(resp.text, 'html.parser')
```

### 1) 영화 제목

```python
# <title>기생충 : 네이버 영화</title>

# title 태그 이름을 활용하여 영화 제목이 포함되어 있는 요소를 찾습니다.
title_tag = soup.find(name='title')
print(title_tag)

# 텍스트 부분만 추출합니다.
title_text = title_tag.get_text()
print(title_text)
```

![스크린샷 2023-02-08 오전 12.17.03.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.17.03.png)

### 2) 리뷰 개수

```python
# <span class="cnt">총<em>110</em>건</span>

# span 태그의 class 속성값을 활용하여 리뷰 갯수가 포함되어 있는 요소를 찾습니다.
count_tag = soup.find(name='span', attrs={'class':'cnt'})
print("span 태그: ", count_tag)

# count_tag 요소에서 em 태그 부분을 찾습니다. 
count_tag = count_tag.find(name='em')
print("em 태그: ", count_tag)

# 텍스트 부분만 추출합니다.
count_text = count_tag.get_text()
print("텍스트: ", count_text)
```

![스크린샷 2023-02-08 오전 12.18.08.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.18.08.png)

### 3) 리뷰 목록

```python
# <ul class="rvw_list_area">

# ul 태그의 class 속성값을 활용하여 리뷰 제목과 링크가 포함되어 있는 요소를 찾습니다.
review_list_tag = soup.find(name='ul', attrs={'class':'rvw_list_area'})

# review_list_tag 요소에 포함된 li 태그를 모두 찾습니다. 
review_list_tags = review_list_tag.find_all(name='li')
print("li 태그의 수: ", len(review_list_tags))
print("\n")

# 첫 번째 원소를 출력합니다. 
print(review_list_tags[0])
```

![첫 번째 리뷰 태그 전체를 출력한 결과 입니다.](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.21.18.png)

첫 번째 리뷰 태그 전체를 출력한 결과 입니다.

```python
review_title = review_list_tags[0].find_all('a')[0].get_text()
print("제목:", review_title, "\n")

review_uid = review_list_tags[0].find_all('a')[1].get_text()
print("사용자:", review_uid, "\n")

review_content = review_list_tags[0].find_all('a')[2].get_text()
print(r"내용:", review_content, "\n")
```

![스크린샷 2023-02-08 오전 12.25.10.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.25.10.png)

```python
review_nid = review_list_tags[0].find('a').get('onclick')
review_nid
```

![스크린샷 2023-02-08 오전 12.26.44.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.26.44.png)

```python
import re

review_nid = re.findall('\d{7}', review_nid)[0]
review_nid
```

![스크린샷 2023-02-08 오전 12.28.19.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.28.19.png)

```python
review_url = f"https://movie.naver.com/movie/bi/mi/reviewread.naver?nid={review_nid}&code=161967&order=#tab"
review_url
```

![스크린샷 2023-02-08 오전 12.35.39.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.35.39.png)

```python
# 10개의 리뷰를 반복문으로 조회하면서, 리뷰 제목(rli.title), 사용자(rli.uid), 상세 페이지 url 값을 추출합니다.
title_list = []
uid_list = []
url_list = []

for li_tag in review_list_tags:
    
    review_title = li_tag.find_all('a')[0].get_text()
    title_list.append(review_title)
    
    review_uid = li_tag.find_all('a')[1].get_text()
    uid_list.append(review_uid)
    
    review_nid = re.findall('\d{7}', li_tag.find('a').get('onclick'))[0]
    review_url = f"https://movie.naver.com/movie/bi/mi/reviewread.naver?nid={review_nid}&code=161967&order=#tab"
    url_list.append(review_url)    
    
# 추출된 아이템의 수량
print(len(title_list))
print(len(uid_list))
```

![스크린샷 2023-02-08 오전 12.38.23.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.38.23.png)

```python
# 리스트 출력
print(url_list)
```

![스크린샷 2023-02-08 오전 12.39.23.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.39.23.png)

### 4) 리뷰 상세 페이지

```python
# 리뷰 상세페이지의 HTML 소스코드를 가져와서 파싱(parsing)
resp_text = requests.get(url_list[0], headers={'User-agent': 'agent'})
soup_text = BeautifulSoup(resp_text.text, 'html.parser')

# 리뷰 본문의 텍스트를 추출합니다. /  <div class="user_tx_area"> )
review_text_tag = soup_text.find(name='div', attrs={'class':'user_tx_area'})

# 텍스트 부분만 추출합니다.
review_text = review_text_tag.get_text()
print(review_text)
```

![스크린샷 2023-02-08 오전 12.51.50.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.51.50.png)

### 5) **10개의 리뷰 본문을 모두 수집 -> List로 정리**

```python
import time

# url 반복하여 텍스트를 추출하고 리스트에 추가

text_list = []

for url in url_list:

    # 리뷰 상세페이지의 HTML 소스코드를 가져와서 파싱(parsing)
    resp_text = requests.get(url, headers={'User-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Safari/605.1.15'})
    soup_text = BeautifulSoup(resp_text.text, 'html.parser')

    # 리뷰 본문의 텍스트를 추출합니다. /  <div class="user_tx_area"> )
    review_text_tag = soup_text.find(name='div', attrs={'class':'user_tx_area'})

    # 텍스트 부분만 추출합니다.
    review_text = review_text_tag.get_text()
    text_list.append(review_text)
    
    # 대기 시간을 추가합니다. (서버에 과도한 호출이 되지 않도록 유의) 
    time.sleep(2)
    
    
# 추출된 아이템의 수량
print(len(text_list))
```

![스크린샷 2023-02-08 오전 12.56.59.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_12.56.59.png)

### 6) **판다스 데이터프레임으로 정리하고, CSV 파일로 저장**

```python
# 딕셔너리 형식으로 항목별 리스트를 원소로 추가 
dict_data = {
    'title' : title_list,
    'user' : uid_list,
    'review' : text_list   
}

# 판다스 데이터프레임으로 변환
import pandas as pd

df_data = pd.DataFrame(dict_data)

# 변환 결과를 확인
df_data
```

![스크린샷 2023-02-08 오전 1.03.25.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_1.03.25.png)

```python
# CSV 파일로 저장
df_data.to_csv("naver_review.csv")
```

# [Part 2] NLP 자연어 처리 및 분석

---

구글 코랩에서 작업했습니다.

```python
# 한글 파일 설치

# (구글 코랩에서 실습할 경우) 아래 코드를 실행하여 한글 폰트를 설치합니다. 
# 설치가 완료되면, 커널을 재시작합니다. (상단 메뉴에서 [런타임] - [런타임 다시 시작] 을 선택)

# (로컬 PC에서 작업할 경우) 이 코드를 실행하지 않습니다!!!

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf
```

![스크린샷 2023-02-08 오전 2.10.27.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.10.27.png)

### 라이브러리 불러오기

```python
import pandas as pd
import numpy as np

import os, re
from tqdm import tqdm

# 경고문구 미표시
import warnings
warnings.filterwarnings('ignore')

# 한글 폰트 지정
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic')
```

### **네이버 영화 리뷰 데이터 - 다운로드**

• 출처: [https://github.com/e9t/nsmc](https://github.com/e9t/nsmc)

```python
# 다운로드 받을 폴더를 준비
DATA_DIR = "./data"
os.makedirs(DATA_DIR, exist_ok=True)
```

```python
# (로컬에서 실습할 경우) 파이토치 설치: conda install pytorch==1.12.0 -c pytorch 
# (로컬에서 실습할 경우) torchtext 설치: conda install -c pytorch torchtext 

# 데이터 다운로드 
import torchtext 
torchtext.utils.download_from_url(url='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt', 
                                  path=os.path.join(DATA_DIR, 'review.txt'))
```

![스크린샷 2023-02-08 오전 2.13.22.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.13.22.png)

```python
# txt 파일을 판다스 데이터프레임으로 읽어오기
data = pd.read_csv(os.path.join(DATA_DIR, 'review.txt'), sep='\t')

# 데이터프레임의 크기
print(data.shape)

# 첫 5행을 출력
data.head()
```

![스크린샷 2023-02-08 오전 2.16.25.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.16.25.png)

## 한글 형태소 분석

### 1) **KoNLPy 설치**

```python
# 코랩에 konlpy 패키지를 설치합니다.
! pip install konlpy
```

### 2) **KoNLPy 한글 형태소 분석**

```python
# 긍정 리뷰를 하나 선택
sample_text = data['document'].iloc[149978]
print(type(sample_text))
print(sample_text)
```

<class 'str'>
그리 만족스럽진못했어도 7점은 나와야되는것같아 10점줌. 주인공들연기도 훌륭했고 내용도 이정도면 괜찮았다. 해피엔딩으로끝났으면 그저 진부한영화가 되버릴뻔. 그래도 엔딩이 섭섭한건 없지않아있었음. 킬러들의도시도그렇고 콜린파렐을 너무 축축하게 끝내버리는듯

```python
# 트위터 형태소 분석기(Okt)를 활용
from konlpy.utils import pprint
from konlpy.tag import Okt
okt = Okt()
print(okt)
```

![스크린샷 2023-02-08 오전 2.30.45.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.30.45.png)

```python
# (단어, 품사) 추출
tokens = okt.pos(sample_text)
pprint(tokens)
```

![스크린샷 2023-02-08 오전 2.32.56.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.32.56.png)

```python
# 단어 ONLY 추출
tokens = okt.morphs(sample_text)
pprint(tokens)
```

![스크린샷 2023-02-08 오전 2.33.10.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.33.10.png)

```python
# 명사 ONLY 추출
tokens = okt.nouns(sample_text)
pprint(tokens)
```

![스크린샷 2023-02-08 오전 2.33.23.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.33.23.png)

### 3) 텍스트 전처리

```python
# 누락 데이터를 제거
review_data = data['document'].dropna().values

# 학습 속도를 고려하여, 1000개의 샘플을 선택하여 추출
review_data = review_data[:1000]

# 배열의 크기
print(review_data.shape)

# 첫 번째 데이터
print(review_data[0])
```

![스크린샷 2023-02-08 오전 2.38.23.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.38.23.png)

```python
# 세 글자 이상의 명사를 사용 (두 글자 이하의 단어는 제거)
cleaned_review_data = []

for review in tqdm(review_data):
    tokens = okt.nouns(review)
    cleaned_tokens = []
    for word in tokens:
        if len(word) > 2:
            cleaned_tokens.append(word)
        else:
            pass
    cleaned_review = " ".join(cleaned_tokens)
    cleaned_review_data.append(cleaned_review)

print(len(cleaned_review_data))
print(cleaned_review_data[0])
```

![스크린샷 2023-02-08 오전 2.39.14.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.39.14.png)

### 4**) TF-IDF 벡터로 표현**

```python
# 사이킷런 패키지 활용 
from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF 변환기 객체를 생성
tfid = TfidfVectorizer()

# TF-IDF 변환기에 데이터를 입력하여 변환
review_tfid = tfid.fit_transform(cleaned_review_data)

# 배열의 크기
print(review_tfid.shape)

# 첫 번째 데이터
print(review_tfid[0])
```

![스크린샷 2023-02-08 오전 2.40.37.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.40.37.png)

```python
# 단어 사전 확인 (딕셔너리 형태)
vocab = tfid.vocabulary_

# 단어 사전의 크기
print(len(vocab))

# 단어 사전 출력 (앞에서 5개의 단어만 출력)
print({ k:v for i, (k, v) in enumerate(vocab.items()) if i < 5 })
```

![스크린샷 2023-02-08 오전 2.42.03.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.42.03.png)

```python
# 단어들의 사전 인덱스를 이용하여 원래 단어를 검색하는 매핑 딕셔너리
index_to_word = { v:k for k, v in vocab.items() } 

# 앞에서 5개의 단어를 출력
print({  k:v for i, (k, v) in enumerate(index_to_word.items()) if i < 5 })
```

![스크린샷 2023-02-08 오전 2.43.31.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.43.31.png)

```python
# 첫 번째 리뷰를 구성하는 단어들의 사전 인덱스를 이용하여 원래 단어를 복원 (순서 복원 X)
original_text = " ".join([index_to_word[word_idx] for word_idx in review_tfid[0].indices])
original_text
```

![스크린샷 2023-02-08 오전 2.44.45.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.44.45.png)

## **Topic Modeling**

> 문서는 여러 종류의 단어들의 집합체로 구성됩니다. 문서를 구성하는 단어들의 확률분포를 활용하여 문서의 토픽(주제)를 파악할 수 있습니다.
> 

### **1) LDA (LatentDirichletAllocation)**

```python
# 사이킷런 패키지 활용
from sklearn.decomposition import LatentDirichletAllocation

# LDA 모델링 객체를 생성 (토픽 갯수를 2로 설정: 긍정/부정)
lda = LatentDirichletAllocation(n_components=2)  

# TF-IDF 벡터를 입력하여 모델 학습 
lda.fit(review_tfid)
```

![스크린샷 2023-02-08 오전 2.55.12.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.55.12.png)

```python
# 토픽 모델링 결과를 담고 있는 배열의 형태 : (2개의 토픽, 2157개의 단어)
print(lda.components_.shape)
```

![스크린샷 2023-02-08 오전 2.56.52.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.56.52.png)

```python
# 2157개의 단어 중에서, 토픽 별로 가장 중요도가 높은 단어를 10개씩 출력

for idx, topic in enumerate(lda.components_):
    print(f"토픽 유형 {idx+1}:", [(index_to_word[i], topic[i].round(3)) for i in topic.argsort()[:-11:-1]])
```

![스크린샷 2023-02-08 오전 2.57.37.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_2.57.37.png)

### **2) pyLDAvis 시각화**

```python
# pyLDAvis 설치
!pip install pyLDAvis

# LDA 토픽 모델링 결과를 시각화
import pyLDAvis.sklearn
pyLDAvis.enable_notebook()
visualization = pyLDAvis.sklearn.prepare(lda, review_tfid, tfid)
pyLDAvis.display(visualization)
```

![스크린샷 2023-02-08 오전 3.01.13.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_3.01.13.png)

## **Sentimental Analysis (감성 분석)**

### **1) 머신러닝 알고리즘을 활용하여, 긍정/부정 감성을 분류**

```python
labels = data['label'].iloc[:1000].values
print(labels.shape)
print(labels[:5])
```

![스크린샷 2023-02-08 오전 3.02.50.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_3.02.50.png)

```python
# 사이킷런 패키지 활용
from sklearn.linear_model import LogisticRegression

# 로지스틱 분류 모델링 객체를 생성 
lr = LogisticRegression()

# TF-IDF 벡터를 입력하여 모델 학습 
lr.fit(review_tfid, labels)
```

![스크린샷 2023-02-08 오전 3.03.17.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_3.03.17.png)

```python
# 첫 번째 샘플을 이용하여 모델 예측
pred = lr.predict(review_tfid[0])
print(pred)
```

![스크린샷 2023-02-08 오전 3.03.49.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_3.03.49.png)

### **2) Part 1에서 수집한 리뷰를 긍정, 부정으로 감성 분류 예측**

코랩 폴더에 CSV 파일을 업로드 (naver_review.csv)

```python
from google.colab import drive
drive.mount('/content/drive')
```

```python
#불러올 파일의 경로를 filename 변수에 저장
filename = '/content/drive/MyDrive/naver_review.csv'
1
2
3
#pandas read_csv로 불러오기
data = pd.read_csv(filename)
data.head()
```

![스크린샷 2023-02-08 오전 3.25.23.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_3.25.23.png)

```python
# 첫 번째 리뷰를 선택
test_sample = test['review'][0]
test_sample
```

![스크린샷 2023-02-08 오전 3.26.42.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_3.26.42.png)

```python
# 한글을 제외하고 전부 제거
test_sample = re.sub(r"[^가-힣]", "", test_sample)
test_sample
```

![스크린샷 2023-02-08 오전 3.27.22.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_3.27.22.png)

```python
# 세 글자 이상의 명사만을 추출

tokens = okt.nouns(test_sample)
cleaned_tokens = []
for word in tokens:
    if len(word) > 2:
        cleaned_tokens.append(word)
    else:
        pass
cleaned_review = " ".join(cleaned_tokens)

cleaned_review
```

![스크린샷 2023-02-08 오전 3.27.54.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_3.27.54.png)

```python
# TF-IDF 변환기에 데이터를 입력하여 변환
test_review_tfid = tfid.transform([cleaned_review])

# 배열을 출력
print(test_review_tfid)
```

![스크린샷 2023-02-08 오전 3.28.28.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_3.28.28.png)

```python
# 로지스틱 회귀 모델을 활용하여 분류 예측
test_pred = lr.predict(test_review_tfid)[0]
print("분석 결과 {}적인 리뷰로 예측됩니다. ".format("긍정" if test_pred > 0 else "부정"))
```

![스크린샷 2023-02-08 오전 3.29.03.png](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7_2023-02-08_%25EC%2598%25A4%25EC%25A0%2584_3.29.03.png)

### 10개 리뷰 분석

```python
for i in range(10):
  test_sample = test['review'][i]
  test_sample = re.sub(r"[^가-힣]", "", test_sample)
  tokens = okt.nouns(test_sample)

  cleaned_tokens = []
  for word in tokens:
    if len(word) > 2:
        cleaned_tokens.append(word)
    else:
        pass
  cleaned_review = " ".join(cleaned_tokens)

  # TF-IDF 변환기에 데이터를 입력하여 변환
  test_review_tfid = tfid.transform([cleaned_review])

  test_pred = lr.predict(test_review_tfid)[0]
  print("분석 결과 {}번째 리뷰는 {}적인 리뷰로 예측됩니다. ".format(i+1, "긍정" if test_pred > 0 else "부정"))
```

## 분석 결과 요약 및 시사점

실습을 통해서 직접 리뷰를 분석해보았을 때, 첫 번째 리뷰의 경우 긍정적인 리뷰로 예측되었다.

그러나 실제 리뷰를 들여다보고, 사람들의 후기를 보면 사뭇 다르다는 것을 알 수 있었다.

이유는 글이 다른 리뷰에 비해 간결하여, 콘텐츠를 제대로 리뷰해내지 못하고 있다는 점, 다른 리뷰에 비해 대표성이 떨어진다는 점을 들 수 있겠다. 무엇보다 리뷰 안에 있는 단어가 긍정적으로 판단을 지을 수 있는 단어로 구성되어 있기 때문으로 보여진다.

로지스틱 회귀 알고리즘 이외의 머신러닝 분류 알고리즘을 공부하여, 다른 관점에서 리뷰를 분석함으로써 정확성을 높혀야 완성도를 높일 수 있다는 생각이 들었다.

# **[추가 과제 예시]**

1. 모델의 예측력을 높이기 위해서, 더 많은 데이터를 모델 학습에 적용해본다. (예제코드에서는 1000개의 데이터를 사용)
2. Hannanum, Mecab 등 다른 종류의 한글 형태소 분석기를 사용해본다.
3. TF-IDF 이외의 다른 종류의 워드 벡터화 기법(word2vec 등)을 적용해본다.
4. LDA 토픽의 갯수를 2개가 아닌 다른 숫자로 적용해본다.
5. 로지스틱 회귀 알고리즘 이외의 머신러닝 분류 알고리즘을 적용해본다.

<pdf>

[202102632 이융의.pdf](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/202102632_%25EC%259D%25B4%25EC%259C%25B5%25EC%259D%2598.pdf)

<영상>

[프레젠테이션1.mp4](3%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5%E1%86%AF%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20(%E1%84%82%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%87%E1%85%A5%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2)%207bf26c968c7f43b3a0c3533746070da8/%25ED%2594%2584%25EB%25A0%2588%25EC%25A0%25A0%25ED%2585%258C%25EC%259D%25B4%25EC%2585%25981.mp4)